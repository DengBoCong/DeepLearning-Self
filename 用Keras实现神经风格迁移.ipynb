{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "用Keras实现神经风格迁移.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4xKz80rEiGXPCrJ3zmIxO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEhKjpsA0Tz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "c66ce91b-e4fc-4851-a112-6ab44fb8add6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "target_image_path = 'sample_data/portrait.png'\n",
        "style_reference_image_path = 'sample_data/transfer_style_reference.png'\n",
        "\n",
        "width, height = load_img(target_image_path).size\n",
        "img_height = 400\n",
        "img_width = int(width * img_height / height)\n",
        "\n",
        "def preprocess_image(image_path):     \n",
        "  img = load_img(image_path, target_size=(img_height, img_width))     \n",
        "  img = img_to_array(img)     \n",
        "  img = np.expand_dims(img, axis=0)     \n",
        "  img = vgg19.preprocess_input(img)     \n",
        "  return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "   x[:, :, 0] += 103.939       \n",
        "   x[:, :, 1] += 116.779     \n",
        "   x[:, :, 2] += 123.68     \n",
        "   x = x[:, :, ::-1]\n",
        "   x = np.clip(x, 0, 255).astype('uint8')\n",
        "   return x\n",
        "\n",
        "target_image = K.constant(preprocess_image(target_image_path))  \n",
        "style_reference_image = K.constant(preprocess_image(style_reference_image_path)) \n",
        "combination_image = K.placeholder((1, img_height, img_width, 3))   \n",
        " \n",
        "input_tensor = K.concatenate([target_image,                                 \n",
        "                style_reference_image,                               \n",
        "                combination_image], axis=0) \n",
        " \n",
        "model = vgg19.VGG19(input_tensor=input_tensor,                       \n",
        "            weights='imagenet',                     \n",
        "            include_top=False) \n",
        "print('Model loaded.')\n",
        "\n",
        "def content_loss(base, combination):     \n",
        "  return K.sum(K.square(combination - base))\n",
        "\n",
        "\n",
        "def gram_matrix(x):     \n",
        "  features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))     \n",
        "  gram = K.dot(features, K.transpose(features))     \n",
        "  return gram \n",
        " \n",
        "def style_loss(style, combination):         \n",
        "  S = gram_matrix(style)         \n",
        "  C = gram_matrix(combination)         \n",
        "  channels = 3         \n",
        "  size = img_height * img_width         \n",
        "  return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2)) \n",
        "\n",
        "def total_variation_loss(x):   \n",
        "  a = K.square(         \n",
        "      x[:, :img_height - 1, :img_width - 1, :] -         \n",
        "      x[:, 1:, :img_width - 1, :])     \n",
        "  b = K.square(         \n",
        "      x[:, :img_height - 1, :img_width - 1, :] -         \n",
        "      x[:, :img_height - 1, 1:, :])     \n",
        "  return K.sum(K.pow(a + b, 1.25))\n",
        "\n",
        "\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "content_layer = 'block5_conv2'   \n",
        "style_layers = ['block1_conv1',\n",
        "         'block2_conv1',                 \n",
        "         'block3_conv1',                 \n",
        "         'block4_conv1',                 \n",
        "         'block5_conv1']\n",
        "\n",
        "total_variation_weight = 1e-4   \n",
        "style_weight = 1. \n",
        "content_weight = 0.025 \n",
        "\n",
        "loss = K.variable(0.)   \n",
        "layer_features = outputs_dict[content_layer] \n",
        "target_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :] \n",
        "loss += content_weight * content_loss(target_image_features,\n",
        "                      combination_features)\n",
        "\n",
        "for layer_name in style_layers:\n",
        "  layer_features = output_dict[layer_name]\n",
        "  style_reference_features = layer_features[1, :, :, 1]\n",
        "  combination_features = layer_features[2, :, :, :]\n",
        "  s1 = style_loss(style_reference_features, combination_features)\n",
        "  loss += (style_weight / len(style_layers)) * s1\n",
        "loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e3ab927bcba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtarget_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mstyle_reference_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_reference_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mcombination_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e3ab927bcba8>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \"\"\"\n\u001b[1;32m    300\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 301\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/transfer_style_reference.png'"
          ]
        }
      ]
    }
  ]
}